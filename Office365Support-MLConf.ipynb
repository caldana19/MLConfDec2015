{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "import sys\n",
    "import scipy\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import sklearn.cross_validation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from  sklearn.cluster import KMeans\n",
    "from sklearn.multiclass  import OneVsRestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cachedStopWords = stopwords.words(\"english\")\n",
    "toker = RegexpTokenizer(r'(\\W)+', gaps=True)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def findTk(s):\n",
    "    s = re.sub(\"[-a-zA-Z0-9._]+@[-a-zA-Z0-9_]+.[a-zA-Z0-9_.]+\", \"EmailAddress\", s)\n",
    "    s = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \"WebAddress\", s)\n",
    "    s = re.sub(\"((?!-)[A-Za-z0-9-]{1,63}(?<!-)\\.)+[A-Za-z]{2,6}\", \"InetDomain\", s)\n",
    "    s = re.sub(\"{?[A-Fa-f0-9]{8}(?:-[A-Fa-f0-9]{4}){3}-[A-Fa-f0-9]{12}}?\", \"CorrelIDGuid\", s)\n",
    "    s = re.sub(\"\\d?\\d/\\d?\\d/\\d\\d\\d\\d \\d?\\d \\d?\\d(\\s\\d?\\d)?(\\s(AM|PM))?\", \"DateTime\", s)\n",
    "    s = re.sub(\"15\\.\\d\\d?\\.\\d\\d?\\d?\\d?(\\.\\d?\\d?\\d?\\d?)?\", \"ProdVersion\", s)\n",
    "    return s\n",
    "\n",
    "def cleanTxt(s, tag):\n",
    "    s = re.sub('(?<=\\s)[-+]?[0-9]*\\.?[0-9]+(?=\\s)', \"NumericData\", s)\n",
    "    tokens = toker.tokenize(s.lower())\n",
    "    r = \"\"\n",
    "    for t in tokens:\n",
    "        i = 0\n",
    "        if not t in cachedStopWords:\n",
    "            if re.match(\"[0-9]+\", t)==None:\n",
    "                st = stemmer.stem(t)\n",
    "                if len(tag)>0:\n",
    "                    r = r + \" \" + tag + st\n",
    "                r = r + \" \" + st\n",
    "    return r.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv('Office365 Training Data.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "df_tr['Route_Label'] = df_tr[['CST_1','CST_2']].apply(lambda x:str(x[0])+'||'+str(x[1]), axis=1)\n",
    "l = df_tr['Route_Label'].unique()  \n",
    "d_CST2cst = {k:v for (k, v) in zip(l, range(len(l)))}\n",
    "f = open(\"O365Support_cst2.pk\", 'wb')\n",
    "pickle.dump(d_CST2cst,f)              \n",
    "f.close()\n",
    "d_cst2CST = {v:k for (k, v) in zip(l, range(len(l)))}\n",
    "f = open(\"O365Support_2cst.pk\", 'wb')\n",
    "pickle.dump(d_cst2CST,f)              \n",
    "f.close()\n",
    "df_tr['Route_Label']=df_tr['Route_Label'].apply(lambda x:d_CST2cst[x])\n",
    "\n",
    "          \n",
    "df_tr.fillna('',inplace=True)\n",
    "df_tr['title'] = df_tr.title.apply(lambda x:findTk(x))\n",
    "df_tr['title'] = df_tr.title.apply(lambda x:cleanTxt(x,\"t_\"))\n",
    "df_tr['problem'] = df_tr.problem.apply(lambda x:findTk(x))\n",
    "df_tr['problem'] = df_tr.problem.apply(lambda x:cleanTxt(x,\"p_\"))\n",
    "df_tr['error'] = df_tr.error.apply(lambda x:findTk(x))\n",
    "df_tr['error'] = df_tr.error.apply(lambda x:cleanTxt(x,\"e_\"))\n",
    "df_tr['txt'] = df_tr[['title','problem','error']].apply(lambda x:str(x[0])+' '+str(x[1])+' '+str(x[2]), axis=1)\n",
    "\n",
    "df_tr['ist1']= df_tr[['IST_1']].apply(lambda x:x[0], axis=1)\n",
    "l = df_tr['ist1'].unique()  \n",
    "d_IST2ist = {k:v for (k, v) in zip(l, range(1,1+len(l)))}\n",
    "df_tr['ist1']=df_tr['ist1'].apply(lambda x:d_IST2ist[x])   \n",
    "\n",
    "f = open(\"O365Support_ist1.pk\", 'wb')\n",
    "pickle.dump(d_IST2ist,f)              \n",
    "f.close()\n",
    "\n",
    "df_tr['ist2']= df_tr[['IST_2']].apply(lambda x:x[0], axis=1)\n",
    "l = df_tr['ist2'].unique()  \n",
    "d_IST2ist = {k:v for (k, v) in zip(l, range(1,1+len(l)))}\n",
    "df_tr['ist2']=df_tr['ist2'].apply(lambda x:d_IST2ist[x]) \n",
    "\n",
    "f = open(\"O365Support_ist2.pk\", 'wb')\n",
    "pickle.dump(d_IST2ist,f)              \n",
    "f.close()\n",
    "\n",
    "f = open(\"O365Support_Encoder.pk\", 'wb')\n",
    "enc = OneHotEncoder()\n",
    "Z = enc.fit_transform(df_tr[['ist1','ist2']].values)\n",
    "pickle.dump(enc,f)              \n",
    "f.close()\n",
    "\n",
    "f = open(\"O365Support_cv1.pk\", 'wb')\n",
    "cv1 = TfidfVectorizer(ngram_range=(1,2), min_df=0.0001)\n",
    "W = cv1.fit_transform(df_tr.txt)\n",
    "pickle.dump(cv1,f)              \n",
    "f.close()\n",
    "\n",
    "X_tr = scipy.sparse.hstack((W,Z), format='csr')\n",
    "Y_tr = df_tr.Route_Label.values\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=15000)\n",
    "X_tr = ch2.fit_transform(X_tr, Y_tr)\n",
    "f = open(\"O365Support_Chi2.pk\", 'wb')\n",
    "pickle.dump(ch2,f)              \n",
    "f.close()\n",
    "\n",
    "clf1 = LinearSVC(C=0.25, tol=1e-10)\n",
    "clf1.fit(X_tr, Y_tr)\n",
    "f = open(\"O365Support_clf1.pk\", 'wb')\n",
    "pickle.dump(clf1,f)              \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tt = pd.read_csv('Office365 Testing Data without Labels.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "df_tt.fillna('',inplace=True)\n",
    "df_tt['title'] = df_tt.title.apply(lambda x:findTk(x))\n",
    "df_tt['title'] = df_tt.title.apply(lambda x:cleanTxt(x,\"t_\"))\n",
    "df_tt['problem'] = df_tt.problem.apply(lambda x:findTk(x))\n",
    "df_tt['problem'] = df_tt.problem.apply(lambda x:cleanTxt(x,\"p_\"))\n",
    "df_tt['error'] = df_tt.error.apply(lambda x:findTk(x))\n",
    "df_tt['error'] = df_tt.error.apply(lambda x:cleanTxt(x,\"e_\"))\n",
    "df_tt['txt'] = df_tt[['title','problem','error']].apply(lambda x:str(x[0])+' '+str(x[1])+' '+str(x[2]), axis=1)\n",
    "\n",
    "f = open(\"O365Support_ist1.pk\", 'rb')\n",
    "d_IST2ist = pickle.load(f)\n",
    "f.close()\n",
    "df_tt['ist1']= df_tt[['IST_1']].apply(lambda x:x[0], axis=1)\n",
    "df_tt['ist1']=df_tt['ist1'].apply(lambda x:d_IST2ist[x] if x in d_IST2ist.keys() else 0)   \n",
    "\n",
    "f = open(\"O365Support_ist2.pk\", 'rb')\n",
    "d_IST2ist = pickle.load(f)\n",
    "f.close()\n",
    "df_tt['ist2']= df_tt[['IST_2']].apply(lambda x:x[0], axis=1)\n",
    "df_tt['ist2']=df_tt['ist2'].apply(lambda x:d_IST2ist[x] if x in d_IST2ist.keys() else 0)   \n",
    "\n",
    "\n",
    "f = open(\"O365Support_Encoder.pk\", 'rb')\n",
    "enc = pickle.load(f)\n",
    "Z = enc.transform(df_tt[['ist1','ist2']].values)\n",
    "f.close()\n",
    "\n",
    "f = open(\"O365Support_cv1.pk\", 'rb')\n",
    "cv1 = pickle.load(f)\n",
    "W = cv1.transform(df_tt.txt)\n",
    "f.close()\n",
    "\n",
    "X_tt = scipy.sparse.hstack((W,Z), format='csr')\n",
    "f = open(\"O365Support_Chi2.pk\", 'rb')\n",
    "ch2 = pickle.load(f)\n",
    "f.close()\n",
    "X_tt = ch2.transform(X_tt)\n",
    "\n",
    "f = open(\"O365Support_clf1.pk\", 'rb')\n",
    "clf1 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"O365Support_2cst.pk\", 'rb')\n",
    "d_cst2CST = pickle.load(f)\n",
    "f.close()\n",
    "df_tt['Scored Labels'] = [d_cst2CST[x] for x in clf1.predict(X_tt)]\n",
    "df_tt['CST_1'] = df_tt['Scored Labels'].apply(lambda x:x.split('||')[0])\n",
    "df_tt['CST_2'] = df_tt['Scored Labels'].apply(lambda x:x.split('||')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tt.to_csv('output.csv', index=False, sep=\"\\t\", columns=['SRId','CST_1','CST_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
